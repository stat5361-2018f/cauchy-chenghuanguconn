---
title: "HW3"
# subtitle: "possible subtitle goes here"
author:
  - Cheng Huang^[<email>; Ph.D. student at
    Department of Statistics, University of Connecticut.]
    
date: "`r format(Sys.time(), '%d %B %Y')`"
documentclass: article
papersize: letter
fontsize: 11pt
bibliography: template.bib
biblio-style: asa
keywords: Monte Carlo, Simulation, .Machine
output:
  pdf_document: default
  html_document:
    df_print: paged
always_allow_html: yes
abstract: |
    Consider estimating the location parameter of a Cauchy distribution with a known scale parameter.
---

## Derivation 1 {#sec:P1}

\begin{align*}
  \ell(\theta)
  &=ln\prod_{i=1}^nf(x_i; \theta)=ln\prod_{i=1}^n\frac{1}{\pi[1+(x_i-\theta)^2]}= -n\ln \pi - \sum_{i=1}^n \ln [1+(\theta-X_i)^2], \\
  \ell'(\theta)
  &= - \sum_{i=1}^n \frac{2(\theta-X_i)}{1+(\theta-X_i)^2}\\
  &= -2 \sum_{i=1}^n \frac{\theta-X_i}{1+(\theta-X_i)^2}, \\
  \ell''(\theta)
  &= -2 \sum_{i=1}^n\frac{(1+(\theta-X_i)^2-2(\theta-X_i)^2)}{[1+(\theta-X_i)^2]^2}\\
  &= -2 \sum_{i=1}^n\frac{1-(\theta-X_i)^2}{[1+(\theta-X_i)^2]^2},
  \\
  I_n(\theta) 
  &=-E[(\ell''(\theta))|\theta]\\
  &=2nE[\frac{1-(\theta-X_i)^2}{(1+(\theta-X_i)^2)^2}]\\
  &=2n\int_{R} \frac{1-(\theta-X_i)^2}{(1+(\theta-X_i)^2)^2}\frac{1}{\pi[1+(\theta-X_i)^2]}dx\\
  &= \frac{4n}{\pi}
  \int_{-\infty}^\infty \frac{x^2\,d x}{(1+x^2)^3}
  = n/2,
\end{align*}
where $I_n$ is the Fisher information of this sample.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) 
require(data.table)
require(knitr)
## require(animation)
require(pracma)
require(ggplot2)

## require(spuRs)  
## fixedpoint ## from spuRs
## require(copula)
```

## generate a random sample
Generate a sample with $n = 10$ and $\theta = 5$.
```{r gen, echo = TRUE}

set.seed(20180909)
n <- 10
par.loc <- 5
data.cauchy <- rcauchy(n, location = par.loc)
```
Implement a loglikelihood function and plot against $\theta$.
```{r llh, echo = TRUE}
llk <- function(loc) {
  sum(dcauchy(data.cauchy, 
              location = loc, 
              log = TRUE))
}

loglikelihood <- Vectorize(llk)
curve(loglikelihood, -20, 20, xname = 'loc')
```


## Newtonâ€“Raphson method
```{r NR, echo = TRUE, message = FALSE, warning=FALSE}
require(ggplot2)
start <- seq(-10, 20, by=0.5)
# newton.method require(animation)
# newtonRaphson require(pracma)
llk.1st <- function(x) {
  -2 * sum((x - data.cauchy)/(1 + (x - data.cauchy) ^ 2))
}
l1 <- Vectorize(llk.1st)
curve(l1, 0, 10, xname = 'loc')
llk.2nd <- function(x){
  -2 * sum((1 - (x - data.cauchy) ^ 2)/((1 + (x - data.cauchy) ^ 2) ^ 2))
}
newtonRaphson
## l2 <- Vectorize(llk.2nd)
## curve(l2, -10, 20, xname = 'loc')
mle.nr <- double(length(start))
iter.nr <- double(length(start)) ## record iteration time
for (i in start){
  NR <- newtonRaphson(fun = llk.1st, x0 = i, dfun = llk.2nd, maxiter = 100)
  mle.nr[which(start == i)] <- NR$root
  iter.nr[which(start == i)] <- NR$niter
}

dt <- data.table(start.value = start, mle = mle.nr, iter.nr = iter.nr)
ggplot(dt, aes(x=start, y=mle.nr)) + geom_point()
kable(dt)
## data.table(cn = names(dt), transpose(dt))
```
## Fixed Point Iteration

```{r FPI, echo = TRUE, message = FALSE, warning=FALSE}
## Function calculate Fixed Point Iteration estimates
FPI <- function(fn, x0, tol = .Machine$double.eps^0.5, maxiter = 100){
    xold <- x0
    xnew <- fn(xold)
    iter <- 1
    while ((abs(xnew - xold) > tol) && (iter < maxiter)) {
        xold <- xnew
        xnew <- fn(xold)
        iter <- iter + 1
    }
    return(list(root = xnew, iter = iter))
}
## function: return result in datatable
rs <- function(alpha, start, ...){
  return(data.table(alpha = alpha,
                    start = start,
                    estimates = FPI(..., x0 = start)$root,
                    iter.fpi = FPI(..., x0 = start)$iter))
}
alpha <- c(1, 0.64, 0.25)

for (i in alpha){
  G <- function(x){
    i * llk.1st(x) + x
  }
    for (j in start){
      if(i == alpha[1] & j ==start[1]){
      tble <- rs(i, j, G)
    }else{
      dt <- rs(i, j, G)
      tble <- rbind(tble, dt)
    }
  }
}
kable(tble)
## data.table(cn = names(tble), transpose(tble))
ggplot(tble, aes(x = start, y = estimates, col = alpha)) + geom_point()
```

## Fisher scoring & Newton
```{r FSNT, echo = TRUE, message = FALSE, warning=FALSE}
## replace l''(theta) with -I(theta)
I <- function(x){
  -length(data.cauchy)/2
}
mle.fs <- double(length(start))
mle.nt <- double(length(start))
itr.fs <- double(length(start))
itr.nt <- double(length(start))
for (i in start){
  mle.fs[which(start == i)] = newtonRaphson(fun = llk.1st, x0 = i, dfun = I, maxiter = 100)$root
  itr.fs[which(start == i)] = newtonRaphson(fun = llk.1st, x0 = i, dfun = I, maxiter = 100)$niter
  mle.nt[which(start == i)] = newtonRaphson(fun = llk.1st, x0 = mle.fs[which(start == i)], dfun = llk.2nd, maxiter = 100)$root
  itr.nt[which(start == i)] = newtonRaphson(fun = llk.1st, x0 = mle.fs[which(start == i)], dfun = llk.2nd, maxiter = 100)$niter
  
}
mle.nt
dt <- data.table(start.value = start, mle.fs = mle.fs, itr.fs = itr.fs, mle.nt = mle.nt, itr.nt = itr.nt)
kable(dt)
```


## Comparison between methods



# Reference {-}

https://stackoverflow.com/questions/28253327/error-with-curve-expr-did-not-evaluate-to-an-object-of-length-n \
[jun-yan/stat-5361]https://github.com/jun-yan/stat-5361 \
[ggplot2 boxplot] \
http://www.sthda.com/english/wiki/ggplot2-box-plot-quick-start-guide-r-software-and-data-visualization
